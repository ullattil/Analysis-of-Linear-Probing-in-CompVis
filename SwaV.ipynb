{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightly split-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import errno\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import urllib.request\n",
    "import random\n",
    "import shutil\n",
    "import copy\n",
    "import math\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning.callbacks import GradientAccumulationScheduler\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.nn import Identity, ModuleList\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import SGD\n",
    "from torchvision.models import resnet50\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets\n",
    "\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.loss import SwaVLoss\n",
    "from lightly.models.modules import SwaVProjectionHead, SwaVPrototypes\n",
    "from lightly.models.modules.memory_bank import MemoryBankModule\n",
    "from lightly.utils.dist import print_rank_zero\n",
    "from lightly.models.utils import (\n",
    "    batch_shuffle,\n",
    "    batch_unshuffle,\n",
    "    get_weight_decay_parameters,\n",
    "    update_momentum,\n",
    "    deactivate_requires_grad\n",
    ")\n",
    "from lightly.transforms import SwaVTransform, utils\n",
    "from lightly.utils.benchmarking import OnlineLinearClassifier\n",
    "from lightly.utils.scheduler import CosineWarmupScheduler\n",
    "from lightly.utils.lars import LARS\n",
    "from imageio import imsave\n",
    "from tqdm import tqdm\n",
    "import splitfolders\n",
    "\n",
    "CROP_COUNTS: Tuple[int, int] = (2, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from google.colab import files\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 2\n",
    "batch_size = 64\n",
    "memory_bank_size = 4096\n",
    "seed = 1\n",
    "max_epochs=200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the STL10 Dataset\n",
    "\n",
    "The STL10 dataset consists of 5000 training images and 8000 test images. To ensure a more effective training of the classification head, we will split the dataset into a 70-15-15 ratio for training, validation, and testing respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stl10_dataset = torchvision.datasets.STL10('/content/stl10/', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 96\n",
    "WIDTH = 96\n",
    "DEPTH = 3\n",
    "\n",
    "SIZE = HEIGHT * WIDTH * DEPTH\n",
    "\n",
    "DATA_DIR = '/content/Datasets'\n",
    "# DATA_URL = 'http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz'\n",
    "\n",
    "TRAIN_DATA_PATH = '/content/stl10/stl10_binary/train_X.bin'\n",
    "TRAIN_LABEL_PATH = '/content/stl10/stl10_binary/train_y.bin'\n",
    "\n",
    "TEST_DATA_PATH = '/content/stl10/stl10_binary/test_X.bin'\n",
    "TEST_LABEL_PATH = '/content/stl10/stl10_binary/test_y.bin'\n",
    "\n",
    "UNLAB_DATA_PATH = '/content/stl10/stl10_binary/unlabeled_X.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_single_image(image_file):\n",
    "\n",
    "  image = np.fromfile(image_file, dtype=np.uint8, count=SIZE)\n",
    "\n",
    "  image = np.reshape(image, (3, 96, 96))\n",
    "  image = np.transpose(image, (2, 1, 0))\n",
    "  return image\n",
    "\n",
    "def plot_image(image):\n",
    "\n",
    "  plt.imshow(image)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_DATA_PATH) as f:\n",
    "  image = read_single_image(f)\n",
    "  plot_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_labels(path_to_labels):\n",
    "\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "\n",
    "def read_all_images(path_to_data):\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "\n",
    "def save_image(image, name):\n",
    "    imsave(\"%s.png\" % name, image, format=\"png\")\n",
    "\n",
    "def save_images(images, labels, types):\n",
    "    i = 0\n",
    "    for image in tqdm(images, position=0):\n",
    "        label = labels[i]\n",
    "        directory = DATA_DIR + '/' + types + '/' + str(label) + '/'\n",
    "        try:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        except OSError as exc:\n",
    "            if exc.errno == errno.EEXIST:\n",
    "                pass\n",
    "        # Append a timestamp to the filename\n",
    "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        filename = directory + str(i) + \"_\" + timestamp\n",
    "        save_image(image, filename)\n",
    "        i = i+1\n",
    "\n",
    "def save_unlabelled_images(images):\n",
    "    i = 0\n",
    "    for image in tqdm(images, position=0):\n",
    "        directory = DATA_DIR + '/' + 'unlabelled' + '/'\n",
    "        try:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        except OSError as exc:\n",
    "            if exc.errno == errno.EEXIST:\n",
    "                pass\n",
    "        filename = directory + str(i)\n",
    "        save_image(image, filename)\n",
    "        i = i+1\n",
    "\n",
    "\n",
    "def create_val_dataset():\n",
    "    train_image_path = DATA_DIR + \"/test\"\n",
    "    folders = os.listdir(train_image_path)\n",
    "\n",
    "    for folder in tqdm(folders, position=0):\n",
    "        temp_dir = DATA_DIR +\"/test/\" + folder\n",
    "        temp_image_list = os.listdir(temp_dir)\n",
    "\n",
    "    for i in range(50):\n",
    "        val_dir = DATA_DIR + \"/val/\" + folder\n",
    "        try:\n",
    "            os.makedirs(val_dir, exist_ok=True)\n",
    "        except OSError as exc:\n",
    "\n",
    "            if exc.errno == errno.EEXIST:\n",
    "                pass\n",
    "        image_name = random.choice(temp_image_list)\n",
    "        temp_image_list.remove(image_name)\n",
    "        old_name = temp_dir + '/' + image_name\n",
    "        new_name = val_dir + '/' + image_name\n",
    "        os.replace(old_name, new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = read_labels(TRAIN_LABEL_PATH)\n",
    "train_images = read_all_images(TRAIN_DATA_PATH)\n",
    "\n",
    "test_labels = read_labels(TEST_LABEL_PATH)\n",
    "test_images = read_all_images(TEST_DATA_PATH)\n",
    "\n",
    "#unlabelled_images = read_all_images(UNLAB_DATA_PATH)\n",
    "# !rm -rf Datasets\n",
    "# save_images(train_images, train_labels, \"test\")\n",
    "# save_images(test_images, test_labels, \"train\")\n",
    "save_images(train_images, train_labels, \"all\")\n",
    "save_images(test_images, test_labels, \"all\")\n",
    "#save_unlabelled_images(unlabelled_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images(data_dir):\n",
    "  # Initialize a counter\n",
    "  num_images = 0\n",
    "\n",
    "  # Iterate through each subdirectory in the data directory\n",
    "  for subdir in os.listdir(data_dir):\n",
    "      sub_dir_path = os.path.join(data_dir, subdir)\n",
    "\n",
    "      # Check if the subdirectory is actually a directory\n",
    "      if os.path.isdir(sub_dir_path):\n",
    "          # Iterate through each file in the subdirectory\n",
    "          for file in os.listdir(sub_dir_path):\n",
    "              # Check if the file is an image\n",
    "              if file.endswith(\".png\"):\n",
    "                  # Increment the counter\n",
    "                  num_images += 1\n",
    "\n",
    "  return num_images\n",
    "\n",
    "print(f\"Number of images: {count_images('Datasets/all')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitfolders.ratio('Datasets/all', output=\"Datasets\", seed=seed, ratio=(.7, .15, .15))\n",
    "!rm -rf Datasets/all/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train = \"/content/Datasets/train\"\n",
    "path_to_test = \"/content/Datasets/test\"\n",
    "path_to_val = \"/content/Datasets/val\"\n",
    "#path_to_unlabelled= \"/content/Datasets/unlabelled\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Pneumonia Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()\n",
    "# Choose the kaggle.json file that you downloaded for the API token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ~/.kaggle\n",
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!cp kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip chest-xray-pneumonia.zip -d chest_xray1\n",
    "!rm -rf chest_xray1/chest_xray/chest_xray\n",
    "!rm -rf chest_xray1/chest_xray/__MACOSX/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir chest_xray\n",
    "!cp -r chest_xray1/chest_xray/train/* -d chest_xray/\n",
    "!cp -r chest_xray1/chest_xray/test/* -d chest_xray/\n",
    "!cp -r chest_xray1/chest_xray/val/* -d chest_xray/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf chest_xray1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def count_images(data_dir):\n",
    "  # Initialize a counter\n",
    "  num_images = 0\n",
    "\n",
    "  # Iterate through each subdirectory in the data directory\n",
    "  for subdir in os.listdir(data_dir):\n",
    "      sub_dir_path = os.path.join(data_dir, subdir)\n",
    "\n",
    "      # Check if the subdirectory is actually a directory\n",
    "      if os.path.isdir(sub_dir_path):\n",
    "          # Iterate through each file in the subdirectory\n",
    "          for file in os.listdir(sub_dir_path):\n",
    "              # Check if the file is an image\n",
    "              if file.endswith(\".jpeg\"):\n",
    "                  # Increment the counter\n",
    "                  num_images += 1\n",
    "\n",
    "  return num_images\n",
    "\n",
    "print(f\"Number of images: {count_images('chest_xray')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "splitfolders.ratio('chest_xray', output=\"xray\", seed=seed, ratio=(.7, .15, .15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf chest_xray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train = \"xray/train\"\n",
    "path_to_test = \"xray/test\"\n",
    "path_to_val = \"xray/val\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the CIFAR-10 Dataset\n",
    "\n",
    "The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. To download the dataset, you can use the following steps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kaggle\n",
    "from google.colab import files\n",
    "files.upload()\n",
    "#upload your API key generated from your profile on kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ~/.kaggle\n",
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!cp kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d swaroopkml/cifar10-pngs-in-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip cifar10-pngs-in-folders.zip -d cifar10-pngs-in-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf cifar10\n",
    "!mkdir cifar10 cifar10_temp cifar10/test\n",
    "!cp -r cifar10-pngs-in-folders/cifar10/cifar10/test/* cifar10/test\n",
    "!cp -r cifar10-pngs-in-folders/cifar10/cifar10/train/* cifar10_temp\n",
    "!rm -rf cifar10-pngs-in-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitfolders.ratio('cifar10_temp', output=\"cifar10\", seed=seed, ratio=(0.8, 0.2), group_prefix=None)\n",
    "!rm -rf cifar10_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_test = \"cifar10/test\"\n",
    "path_to_train = \"cifar10/train\"\n",
    "path_to_val = \"cifar10/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def count_images(data_dir):\n",
    "  # Initialize a counter\n",
    "  num_images = 0\n",
    "\n",
    "  # Iterate through each subdirectory in the data directory\n",
    "  for subdir in os.listdir(data_dir):\n",
    "      sub_dir_path = os.path.join(data_dir, subdir)\n",
    "\n",
    "      # Check if the subdirectory is actually a directory\n",
    "      if os.path.isdir(sub_dir_path):\n",
    "          # Iterate through each file in the subdirectory\n",
    "          for file in os.listdir(sub_dir_path):\n",
    "              # Check if the file is an image\n",
    "              if file.endswith(\".png\"):\n",
    "                  # Increment the counter\n",
    "                  num_images += 1\n",
    "\n",
    "  return num_images\n",
    "\n",
    "print(f\"Number of images: {count_images(path_to_train)}\")\n",
    "print(f\"Number of images: {count_images(path_to_test)}\")\n",
    "print(f\"Number of images: {count_images(path_to_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentations and Transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = SwaVTransform(crop_counts=CROP_COUNTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classifier_transforms = transforms.Compose(\n",
    "    [\n",
    "      transforms.Resize((224, 224)),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "      transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# No additional augmentations for the test set\n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_SwaV = LightlyDataset(input_dir=path_to_train, transform=transform)\n",
    "\n",
    "dataset_train_classifier = LightlyDataset(\n",
    "    input_dir=path_to_train, transform=train_classifier_transforms\n",
    ")\n",
    "\n",
    "dataset_test = LightlyDataset(input_dir=path_to_test, transform=test_transforms)\n",
    "\n",
    "dataset_val = LightlyDataset(input_dir=path_to_val, transform=test_transforms)\n",
    "\n",
    "\n",
    "# unlabelled_dataset_train = LightlyDataset(\n",
    "#     input_dir=path_to_unlabelled, transform=train_classifier_transforms\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train_SwaV = torch.utils.data.DataLoader(\n",
    "    dataset_train_SwaV,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "dataloader_train_classifier = torch.utils.data.DataLoader(\n",
    "    dataset_train_classifier,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "dataloader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "# dataloader_unlabelled = torch.utils.data.DataLoader(\n",
    "#     unlabelled_dataset_train,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True,\n",
    "#     drop_last=True,\n",
    "#     num_workers=num_workers,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwAV(LightningModule):\n",
    "    def __init__(self, batch_size_per_device: int=64, num_classes: int=10) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.batch_size_per_device = batch_size_per_device\n",
    "\n",
    "        resnet = resnet50()\n",
    "        resnet.fc = Identity()  # Ignore classification head\n",
    "        self.backbone = resnet\n",
    "        self.projection_head = SwaVProjectionHead()\n",
    "        self.prototypes = SwaVPrototypes(n_steps_frozen_prototypes=1)\n",
    "        self.criterion = SwaVLoss(sinkhorn_gather_distributed=True)\n",
    "        self.online_classifier = OnlineLinearClassifier(num_classes=num_classes)\n",
    "\n",
    "        # Use a queue for small batch sizes (<= 256).\n",
    "        self.start_queue_at_epoch = 15\n",
    "        self.n_batches_in_queue = 15\n",
    "        self.queues = ModuleList(\n",
    "            [\n",
    "                MemoryBankModule(\n",
    "                    size=(self.n_batches_in_queue * self.batch_size_per_device, 128)\n",
    "                )\n",
    "                for _ in range(CROP_COUNTS[0])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def project(self, x: Tensor) -> Tensor:\n",
    "        x = self.projection_head(x)\n",
    "        return F.normalize(x, dim=1, p=2)\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: Tuple[List[Tensor], Tensor, List[str]], batch_idx: int\n",
    "    ) -> Tensor:\n",
    "        # Normalize the prototypes so they are on the unit sphere.\n",
    "        self.prototypes.normalize()\n",
    "\n",
    "        # The dataloader returns a list of image crops where the\n",
    "        # first few items are high resolution crops and the rest are low\n",
    "        # resolution crops.\n",
    "        multi_crops, targets = batch[0], batch[1]\n",
    "\n",
    "        # Forward pass through backbone and projection head.\n",
    "        multi_crop_features = [\n",
    "            self.forward(crops).flatten(start_dim=1) for crops in multi_crops\n",
    "        ]\n",
    "        multi_crop_projections = [\n",
    "            self.project(features) for features in multi_crop_features\n",
    "        ]\n",
    "\n",
    "        # Get the queue projections and logits.\n",
    "        queue_crop_logits = None\n",
    "        with torch.no_grad():\n",
    "            if self.current_epoch >= self.start_queue_at_epoch:\n",
    "                # Start filling the queue.\n",
    "                queue_crop_projections = _update_queue(\n",
    "                    projections=multi_crop_projections[: CROP_COUNTS[0]],\n",
    "                    queues=self.queues,\n",
    "                )\n",
    "                if batch_idx > self.n_batches_in_queue:\n",
    "                    # The queue is filled, so we can start using it.\n",
    "                    queue_crop_logits = [\n",
    "                        self.prototypes(projections, step=self.current_epoch)\n",
    "                        for projections in queue_crop_projections\n",
    "                    ]\n",
    "\n",
    "        # Get the rest of the multi-crop logits.\n",
    "        multi_crop_logits = [\n",
    "            self.prototypes(projections, step=self.current_epoch)\n",
    "            for projections in multi_crop_projections\n",
    "        ]\n",
    "\n",
    "        # Calculate the SwAV loss.\n",
    "        loss = self.criterion(\n",
    "            high_resolution_outputs=multi_crop_logits[: CROP_COUNTS[0]],\n",
    "            low_resolution_outputs=multi_crop_logits[CROP_COUNTS[0] :],\n",
    "            queue_outputs=queue_crop_logits,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "            batch_size=len(targets),\n",
    "        )\n",
    "\n",
    "        # Calculate the classification loss.\n",
    "        cls_loss, cls_log = self.online_classifier.training_step(\n",
    "            (multi_crop_features[0].detach(), targets), batch_idx\n",
    "        )\n",
    "        self.log_dict(cls_log, sync_dist=True, batch_size=len(targets))\n",
    "        return loss + cls_loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: Tuple[Tensor, Tensor, List[str]], batch_idx: int\n",
    "    ) -> Tensor:\n",
    "        images, targets = batch[0], batch[1]\n",
    "        features = self.forward(images).flatten(start_dim=1)\n",
    "        cls_loss, cls_log = self.online_classifier.validation_step(\n",
    "            (features.detach(), targets), batch_idx\n",
    "        )\n",
    "        self.log_dict(cls_log, prog_bar=True, sync_dist=True, batch_size=len(targets))\n",
    "        return cls_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Don't use weight decay for batch norm, bias parameters, and classification\n",
    "        # head to improve performance.\n",
    "        params, params_no_weight_decay = get_weight_decay_parameters(\n",
    "            [self.backbone, self.projection_head, self.prototypes]\n",
    "        )\n",
    "        optimizer = LARS(\n",
    "            [\n",
    "                {\"name\": \"swav\", \"params\": params},\n",
    "                {\n",
    "                    \"name\": \"swav_no_weight_decay\",\n",
    "                    \"params\": params_no_weight_decay,\n",
    "                    \"weight_decay\": 0.0,\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"online_classifier\",\n",
    "                    \"params\": self.online_classifier.parameters(),\n",
    "                    \"weight_decay\": 0.0,\n",
    "                },\n",
    "            ],\n",
    "            # Smaller learning rate for smaller batches: lr=0.6 for batch_size=256\n",
    "            # scaled linearly by batch size to lr=4.8 for batch_size=2048.\n",
    "            # See Appendix A.1. and A.6. in SwAV paper https://arxiv.org/pdf/2006.09882.pdf\n",
    "            lr=0.6 * (self.batch_size_per_device * self.trainer.world_size) / 256,\n",
    "            momentum=0.9,\n",
    "            weight_decay=1e-6,\n",
    "        )\n",
    "        scheduler = {\n",
    "            \"scheduler\": CosineWarmupScheduler(\n",
    "                optimizer=optimizer,\n",
    "                warmup_epochs=int(\n",
    "                    self.trainer.estimated_stepping_batches\n",
    "                    / self.trainer.max_epochs\n",
    "                    * 10\n",
    "                ),\n",
    "                max_epochs=int(self.trainer.estimated_stepping_batches),\n",
    "                end_value=0.0006\n",
    "                * (self.batch_size_per_device * self.trainer.world_size)\n",
    "                / 256,\n",
    "            ),\n",
    "            \"interval\": \"step\",\n",
    "        }\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "transform = SwaVTransform(crop_counts=CROP_COUNTS)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def _update_queue(\n",
    "    projections: List[Tensor],\n",
    "    queues: ModuleList,\n",
    "):\n",
    "    \"\"\"Adds the high resolution projections to the queues and returns the queues.\"\"\"\n",
    "\n",
    "    if len(projections) != len(queues):\n",
    "        raise ValueError(\n",
    "            f\"The number of queues ({len(queues)}) should be equal to the number of high \"\n",
    "            f\"resolution inputs ({len(projections)}).\"\n",
    "        )\n",
    "\n",
    "    # Get the queue projections\n",
    "    queue_projections = []\n",
    "    for i in range(len(queues)):\n",
    "        _, queue_proj = queues[i](projections[i], update=True)\n",
    "        # Queue projections are in (num_ftrs X queue_length) shape, while the high res\n",
    "        # projections are in (batch_size_per_device X num_ftrs). Swap the axes for interoperability.\n",
    "        queue_proj = torch.permute(queue_proj, (1, 0))\n",
    "        queue_projections.append(queue_proj)\n",
    "\n",
    "    return queue_projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Probing Classifier ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(pl.LightningModule):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        # use the pretrained ResNet backbone\n",
    "        self.backbone = backbone\n",
    "\n",
    "        # freeze the backbone\n",
    "        deactivate_requires_grad(backbone)\n",
    "\n",
    "        # create a linear layer for our downstream classification model\n",
    "        self.fc = nn.Linear(2048, 10)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_hat = self.backbone(x).flatten(start_dim=1)\n",
    "        y_hat = self.fc(y_hat)\n",
    "        return y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"train_loss_fc\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch  # Assuming batch contains input data (x), labels (y), and any additional information (_)\n",
    "        y_hat = self.forward(x)  # Forward pass to get predictions\n",
    "\n",
    "        # Compute accuracy\n",
    "        predictions = torch.argmax(y_hat, dim=1)  # Get predicted labels\n",
    "        correct_predictions = (predictions == y).sum().item()  # Count correct predictions\n",
    "        total_samples = len(y)  # Total number of samples in the batch\n",
    "        accuracy = correct_predictions / total_samples  # Calculate accuracy\n",
    "\n",
    "        self.log(\"test_accuracy\", accuracy, on_step=False, on_epoch=True)  # Log accuracy for tracking\n",
    "\n",
    "        return accuracy  # Return accuracy\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.custom_histogram_weights()\n",
    "\n",
    "    # We provide a helper method to log weights in tensorboard\n",
    "    # which is useful for debugging.\n",
    "    def custom_histogram_weights(self):\n",
    "        for name, params in self.named_parameters():\n",
    "            self.logger.experiment.add_histogram(name, params, self.current_epoch)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = torch.nn.functional.softmax(y_hat, dim=1)\n",
    "\n",
    "        # calculate number of correct predictions\n",
    "        _, predicted = torch.max(y_hat, 1)\n",
    "        num = predicted.shape[0]\n",
    "        correct = (predicted == y).float().sum()\n",
    "        self.validation_step_outputs.append((num, correct))\n",
    "        return num, correct\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # calculate and log top1 accuracy\n",
    "        if self.validation_step_outputs:\n",
    "            total_num = 0\n",
    "            total_correct = 0\n",
    "            for num, correct in self.validation_step_outputs:\n",
    "                total_num += num\n",
    "                total_correct += correct\n",
    "            acc = total_correct / total_num\n",
    "            self.log(\"val_acc\", acc, on_epoch=True, prog_bar=True)\n",
    "            self.validation_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(self.fc.parameters(), lr=0.03, momentum=0.9, weight_decay=0.0005)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Pretraining Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# import torchvision.transforms as T\n",
    "\n",
    "# batch_size_per_device = 64\n",
    "# num_classes = 1000\n",
    "# train_dataset = LightlyDataset(input_dir=path_to_train, transform=transform)\n",
    "# train_dataloader = DataLoader(\n",
    "#         train_dataset,\n",
    "#         batch_size=batch_size_per_device,\n",
    "#         shuffle=True,\n",
    "#         num_workers=num_workers,\n",
    "#         drop_last=True,\n",
    "#         persistent_workers=False,\n",
    "# )\n",
    "# # Setup validation data.\n",
    "# val_transform = T.Compose(\n",
    "#     [\n",
    "#         T.Resize(256),\n",
    "#         T.CenterCrop(224),\n",
    "#         T.ToTensor(),\n",
    "#         T.Normalize(mean=[0.4467106, 0.43980986, 0.40664646], std=[0.26034098, 0.25657727, 0.27126738]),\n",
    "#     ]\n",
    "# )\n",
    "# val_dataset = LightlyDataset(input_dir=str(path_to_val), transform=val_transform)\n",
    "# val_dataloader = DataLoader(\n",
    "#     val_dataset,\n",
    "#     batch_size=batch_size_per_device,\n",
    "#     shuffle=False,\n",
    "#     num_workers=num_workers,\n",
    "#     persistent_workers=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightly.utils.benchmarking import MetricCallback\n",
    "# from pytorch_lightning.callbacks import (\n",
    "#     DeviceStatsMonitor,\n",
    "#     EarlyStopping,\n",
    "#     LearningRateMonitor,\n",
    "# )\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "# # Check if CUDA (GPU) is available\n",
    "# os.environ['RANK'] = '0'\n",
    "# os.environ['WORLD_SIZE'] = '1'\n",
    "# os.environ['MASTER_ADDR'] = 'localhost'\n",
    "# os.environ['MASTER_PORT'] = '12355'\n",
    "# if torch.cuda.is_available():\n",
    "#     # Initialize the default process group\n",
    "#     torch.distributed.init_process_group(backend='nccl')\n",
    "#     # Now you can perform your training using PyTorch DistributedDataParallel\n",
    "#     # or other distributed training techniques\n",
    "# else:\n",
    "#     print(\"CUDA is not available. Cannot perform distributed training.\")\n",
    "\n",
    "# accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# metric_callback = MetricCallback()\n",
    "# trainer = pl.Trainer(\n",
    "#         max_epochs=102,\n",
    "#         accelerator=accelerator,\n",
    "#         devices=1,\n",
    "#         callbacks=[\n",
    "#             LearningRateMonitor(),\n",
    "#             # Stop if training loss diverges.\n",
    "#             EarlyStopping(monitor=\"train_loss\", patience=int(1e12), check_finite=True),\n",
    "#             DeviceStatsMonitor(),\n",
    "#             metric_callback,\n",
    "#         ],\n",
    "#         logger=TensorBoardLogger(save_dir=\"/content/logs/\", name=\"pretrain\"),\n",
    "#         precision=\"16-mixed\",\n",
    "#         sync_batchnorm=accelerator != \"cpu\",  # Sync batchnorm is not supported on CPU.\n",
    "#         num_sanity_val_steps=0,\n",
    "# )\n",
    "\n",
    "# model = MoCoV2(batch_size_per_device=batch_size_per_device, num_classes=num_classes)\n",
    "# trainer.fit(\n",
    "#         model=model,\n",
    "#         train_dataloaders=train_dataloader,\n",
    "#         val_dataloaders=val_dataloader,\n",
    "#         ckpt_path=ckpt_path,\n",
    "# )\n",
    "# for metric in [\"val_online_cls_top1\", \"val_online_cls_top5\"]:\n",
    "#         print_rank_zero(f\"max {metric}: {max(metric_callback.val_metrics[metric])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!cp \"/content/drive/MyDrive/Colab Notebooks/swav_epoch-99.ckpt\" \"/content/swav_epoch-99.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"/content/swav_epoch-99.ckpt\"\n",
    "model = SwAV.load_from_checkpoint(\"/content/swav_epoch-99.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # copy the classifier checkpoints from drive\n",
    "# !cp \"drive/My Drive/Colab Notebooks/classifier_model_stl10_200.ckpt\" \"/content/classifier_model_stl10_200.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# till 5th epoch, it will accumulate every 8 batches. From 5th epoch\n",
    "# till 9th epoch it will accumulate every 4 batches and after that no accumulation\n",
    "# will happen. Note that you need to use zero-indexed epoch keys here\n",
    "accumulator = GradientAccumulationScheduler(scheduling={0: 10, 4: 5, 8: 2})\n",
    "\n",
    "classifier_trainer = pl.Trainer(\n",
    "    max_epochs=100,  # Adjust the number of epochs as needed\n",
    "    devices=1,\n",
    "    accelerator=\"gpu\",\n",
    "    callbacks=accumulator,\n",
    ")\n",
    "\n",
    "classifier=Classifier(model.backbone)\n",
    "\n",
    "# classifier_trainer.fit(\n",
    "#     classifier,\n",
    "#     dataloader_train_classifier,\n",
    "#     dataloader_val,\n",
    "#     #ckpt_path=\"classifier_model_cifar10.ckpt\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model and checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_trainer.save_checkpoint(\"classifier_swav_cifar10_100.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp \"classifier_swav_cifar10_100.ckpt\" \"drive/My Drive/Colab Notebooks/classifier_swav_cifar10_100.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r lightning_logs_swav_cifar10.zip /content/lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp \"/content/lightning_logs_swav_cifar10.zip\" \"drive/My Drive/lightning_logs_swav_cifar10.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp \"drive/My Drive/Colab Notebooks/classifier_swav_cifar10_100.ckpt\" \"classifier_swav_cifar10_100.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_trainer.test(classifier, dataloader_train_classifier, ckpt_path=\"classifier_swav_cifar10_100.ckpt\") #training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_trainer.test(classifier, dataloader_test, ckpt_path=\"classifier_swav_cifar10_100.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and view logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#connect to google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r lightning_logs_SwaV_stl_10.zip /content/lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp \"drive/My Drive/Colab Notebooks/classifier_model_SwaV_stl10.ckpt\" \"/content/classifier_model_stl10_200.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp \"drive/My Drive/lightning_logs_swav_cifar10.zip\" \"/content/lightning_logs_SwaV_stl_10.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip \"lightning_logs_SwaV_stl_10.zip\" -d \"/content/lightning_logs1\"\n",
    "!mv lightning_logs1/content/* .\n",
    "!rm -rf lightning_logs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
